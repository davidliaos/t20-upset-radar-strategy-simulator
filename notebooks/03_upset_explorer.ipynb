{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 03 - Upset Explorer\n",
        "\n",
        "Analyze where underdog wins cluster and how upset rates change across context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "PROJECT_ROOT = Path.cwd().resolve().parent if Path.cwd().name == \"notebooks\" else Path.cwd().resolve()\n",
        "if str(PROJECT_ROOT) not in sys.path:\n",
        "    sys.path.insert(0, str(PROJECT_ROOT))\n",
        "\n",
        "from src.data_prep import load_matches, build_team1_win_target, assign_favorite_underdog_from_elo\n",
        "from src.viz import upset_rate_by_bucket"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = load_matches()\n",
        "df = build_team1_win_target(df)\n",
        "df = assign_favorite_underdog_from_elo(df)\n",
        "print(f\"Rows: {len(df):,}\")\n",
        "print(f\"Overall upset rate: {df['is_upset'].mean():.2%}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bucket_table = upset_rate_by_bucket(df, score_col=\"elo_diff\", upset_col=\"is_upset\", bins=10)\n",
        "bucket_table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e454f62",
      "metadata": {},
      "outputs": [],
      "source": [
        "stage_venue = (\n",
        "    df.groupby([\"match_stage\", \"venue\"], observed=True)[\"is_upset\"]\n",
        "    .mean()\n",
        "    .reset_index(name=\"upset_rate\")\n",
        ")\n",
        "top_venues = (\n",
        "    df[\"venue\"].value_counts().head(12).index.tolist()\n",
        ")\n",
        "heat = stage_venue[stage_venue[\"venue\"].isin(top_venues)].pivot(\n",
        "    index=\"match_stage\", columns=\"venue\", values=\"upset_rate\"\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(14, 6))\n",
        "sns.heatmap(heat, cmap=\"viridis\", annot=False)\n",
        "plt.title(\"Upset Rate by Match Stage and Venue (Top Venues)\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "toss_stage = (\n",
        "    df.groupby([\"match_stage\", \"toss_decision\"], observed=True)[\"is_upset\"]\n",
        "    .mean()\n",
        "    .reset_index(name=\"upset_rate\")\n",
        ")\n",
        "toss_stage.sort_values([\"match_stage\", \"upset_rate\"], ascending=[True, False]).head(30)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4d4db65",
      "metadata": {},
      "source": [
        "## SHAP Global Feature Importance\n",
        "\n",
        "Train a compact model to predict team1 win and compute SHAP global feature importance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1590864b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# SHAP global explanation: pipeline pattern from src, compact model, fallback if SHAP unavailable\n",
        "# Requires df from previous cells.\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "from src.data_prep import time_based_split\n",
        "from src.features import build_pre_match_feature_frame\n",
        "from src.models import train_logistic_baseline\n",
        "\n",
        "train_df, valid_df, test_df = time_based_split(df)\n",
        "X_train, y_train = build_pre_match_feature_frame(train_df)\n",
        "X_valid, y_valid = build_pre_match_feature_frame(valid_df)\n",
        "\n",
        "# Compact logistic model for explainability\n",
        "shap_model = train_logistic_baseline(X_train, y_train)\n",
        "prob_valid = shap_model.predict_proba(X_valid)[:, 1]\n",
        "print(\"Logistic baseline trained for SHAP (validation ROC-AUC):\",\n",
        "      round(roc_auc_score(y_valid, prob_valid), 4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5bcf9249",
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    import numpy as np\n",
        "    import shap\n",
        "\n",
        "    preprocess = shap_model.named_steps[\"preprocess\"]\n",
        "    clf = shap_model.named_steps[\"clf\"]\n",
        "    X_transformed = preprocess.transform(X_train)\n",
        "    feature_names = preprocess.get_feature_names_out().tolist()\n",
        "\n",
        "    sample_size = min(200, len(X_transformed))\n",
        "    rng = np.random.default_rng(42)\n",
        "    idx = rng.choice(len(X_transformed), size=sample_size, replace=False)\n",
        "    X_bg = X_transformed[idx]\n",
        "\n",
        "    eval_size = min(500, len(X_transformed))\n",
        "    eval_idx = rng.choice(len(X_transformed), size=eval_size, replace=False)\n",
        "    X_eval = X_transformed[eval_idx]\n",
        "\n",
        "    explainer = shap.LinearExplainer(clf, X_bg, feature_names=feature_names)\n",
        "    shap_vals = explainer.shap_values(X_eval)\n",
        "    if isinstance(shap_vals, list):\n",
        "        shap_vals = shap_vals[1]  # positive class for binary\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 8))\n",
        "    shap.summary_plot(shap_vals, X_eval, feature_names=feature_names, plot_type=\"bar\", show=False, max_display=15)\n",
        "    plt.title(\"SHAP Global Feature Importance (Team1 Win Model)\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "except Exception as e:\n",
        "    print(f\"SHAP not available or failed in this environment: {e}\")\n",
        "    print(\"Install with: pip install shap\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "850a5636",
      "metadata": {},
      "source": [
        "## Local Upset Explanations (Counterfactual)\n",
        "\n",
        "Instead of only ranking feature importance globally, inspect one upset at a time with domain-specific counterfactual changes (toss decision, ELO gap neutralization, form neutralization, stage shift)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f522ef6e",
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.explain import build_counterfactual_explanation, rank_notable_upsets\n",
        "\n",
        "# Reuse the logistic model from SHAP cells if present; otherwise train a quick one.\n",
        "if \"shap_model\" not in globals():\n",
        "    from src.data_prep import time_based_split\n",
        "    from src.features import build_pre_match_feature_frame\n",
        "    from src.models import train_logistic_baseline\n",
        "\n",
        "    train_df, valid_df, _ = time_based_split(df)\n",
        "    X_train_local, y_train_local = build_pre_match_feature_frame(train_df)\n",
        "    shap_model = train_logistic_baseline(X_train_local, y_train_local)\n",
        "\n",
        "notable = rank_notable_upsets(df, top_n=8)\n",
        "notable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cdeca7d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pick the highest-ELO-gap upset and explain it with counterfactuals.\n",
        "if len(notable) > 0:\n",
        "    top_case = notable.iloc[0]\n",
        "    team1_case = top_case[\"team1\"]\n",
        "    team2_case = top_case[\"team2\"]\n",
        "    case_rows = df[(df[\"team1\"] == team1_case) & (df[\"team2\"] == team2_case)].copy()\n",
        "\n",
        "    if case_rows.empty:\n",
        "        print(\"No direct row found for selected upset case in team1/team2 orientation.\")\n",
        "    else:\n",
        "        # Build a model-ready row from the selected case.\n",
        "        cols = [\n",
        "            \"team1\", \"team2\", \"match_stage\", \"venue\", \"toss_winner\", \"toss_decision\",\n",
        "            \"elo_team1\", \"elo_team2\", \"elo_diff\", \"team1_form_5\", \"team2_form_5\",\n",
        "            \"team1_form_10\", \"team2_form_10\", \"h2h_win_pct\"\n",
        "        ]\n",
        "        base_row = case_rows[cols].head(1)\n",
        "        local_exp = build_counterfactual_explanation(shap_model, base_row)\n",
        "        print(\"Base team1 win probability:\", round(local_exp[\"base_team1_win_prob\"], 4))\n",
        "        pd.DataFrame(local_exp[\"counterfactuals\"]) \n",
        "else:\n",
        "    print(\"No upset cases available for local explanation.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
