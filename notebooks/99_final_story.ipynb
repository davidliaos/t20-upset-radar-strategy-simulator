{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# T20 Upset Radar and Strategy Simulator: Final Story\n",
        "\n",
        "This notebook is the portfolio-ready narrative for the MVP.\n",
        "\n",
        "It walks through:\n",
        "1. Data scope and quality snapshot\n",
        "2. Modeling and calibration baseline\n",
        "3. Upset-focused evaluation\n",
        "4. Explainability and local upset narratives\n",
        "5. Strategy simulator examples\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
        "\n",
        "from src.config import PROCESSED_DIR\n",
        "from src.data_prep import (\n",
        "    assign_favorite_underdog_from_elo,\n",
        "    build_team1_win_target,\n",
        "    load_matches,\n",
        "    time_based_split,\n",
        ")\n",
        "from src.explain import build_counterfactual_explanation, rank_notable_upsets\n",
        "from src.features import build_pre_match_feature_frame\n",
        "from src.models import calibrate_classifier, evaluate_binary_model, train_logistic_baseline\n",
        "from src.simulation import ScenarioInput, build_scenario_features, score_scenario\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Data Scope and Quality Snapshot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = load_matches()\n",
        "df = build_team1_win_target(df)\n",
        "df = assign_favorite_underdog_from_elo(df)\n",
        "\n",
        "quality_path = PROCESSED_DIR / \"data_quality_report.json\"\n",
        "quality = {}\n",
        "if quality_path.exists():\n",
        "    quality = json.loads(quality_path.read_text())\n",
        "\n",
        "print(\"Rows:\", len(df))\n",
        "print(\"Date range:\", df[\"date\"].min(), \"->\", df[\"date\"].max())\n",
        "print(\"Unique teams:\", df[\"team1\"].nunique())\n",
        "print(\"Overall upset rate:\", round(float(df[\"is_upset\"].mean()), 4))\n",
        "if quality:\n",
        "    print(\"Duplicate match_ids:\", quality.get(\"duplicate_match_id_count\", \"n/a\"))\n",
        "    print(\"Columns with missing values:\", quality.get(\"columns_with_missing_values\", \"n/a\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Baseline Modeling and Calibration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_df, valid_df, test_df = time_based_split(df)\n",
        "\n",
        "X_train, y_train = build_pre_match_feature_frame(train_df)\n",
        "X_valid, y_valid = build_pre_match_feature_frame(valid_df)\n",
        "X_test, y_test = build_pre_match_feature_frame(test_df)\n",
        "\n",
        "base_model = train_logistic_baseline(X_train, y_train)\n",
        "calibrated_model = calibrate_classifier(base_model, X_valid, y_valid)\n",
        "test_metrics = evaluate_binary_model(calibrated_model, X_test, y_test)\n",
        "pd.Series(test_metrics).round(4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Upset-Focused Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_eval = test_df.copy()\n",
        "test_eval = assign_favorite_underdog_from_elo(test_eval)\n",
        "X_eval, _ = build_pre_match_feature_frame(test_eval)\n",
        "test_eval[\"team1_win_prob\"] = calibrated_model.predict_proba(X_eval)[:, 1]\n",
        "test_eval[\"pred_team1_win\"] = (test_eval[\"team1_win_prob\"] >= 0.5).astype(int)\n",
        "\n",
        "# Predict winner names in match orientation.\n",
        "test_eval[\"pred_winner\"] = test_eval.apply(\n",
        "    lambda r: r[\"team1\"] if r[\"pred_team1_win\"] == 1 else r[\"team2\"], axis=1\n",
        ")\n",
        "test_eval[\"pred_is_upset\"] = (test_eval[\"pred_winner\"] == test_eval[\"underdog_team\"]).astype(int)\n",
        "\n",
        "upset_cm = confusion_matrix(test_eval[\"is_upset\"], test_eval[\"pred_is_upset\"])\n",
        "upset_prf = precision_recall_fscore_support(\n",
        "    test_eval[\"is_upset\"], test_eval[\"pred_is_upset\"], average=\"binary\", zero_division=0\n",
        ")\n",
        "\n",
        "print(\"Upset confusion matrix (rows=true, cols=pred):\")\n",
        "print(upset_cm)\n",
        "print(\"Upset precision/recall/f1:\", tuple(round(float(x), 4) for x in upset_prf[:3]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Explainability: Notable Upsets + Counterfactual View"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "notable = rank_notable_upsets(df, top_n=8)\n",
        "notable[[\"date\", \"team1\", \"team2\", \"winner\", \"elo_diff\"]].head(8)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if len(notable) > 0:\n",
        "    top_case = notable.iloc[0]\n",
        "    case_df = df[(df[\"team1\"] == top_case[\"team1\"]) & (df[\"team2\"] == top_case[\"team2\"])].copy()\n",
        "    cols = [\n",
        "        \"team1\", \"team2\", \"match_stage\", \"venue\", \"toss_winner\", \"toss_decision\",\n",
        "        \"elo_team1\", \"elo_team2\", \"elo_diff\", \"team1_form_5\", \"team2_form_5\",\n",
        "        \"team1_form_10\", \"team2_form_10\", \"h2h_win_pct\"\n",
        "    ]\n",
        "    if not case_df.empty:\n",
        "        local_exp = build_counterfactual_explanation(calibrated_model, case_df[cols].head(1))\n",
        "        print(\"Base team1 win probability:\", round(local_exp[\"base_team1_win_prob\"], 4))\n",
        "        pd.DataFrame(local_exp[\"counterfactuals\"])\n",
        "    else:\n",
        "        print(\"No direct oriented row found for the top upset case.\")\n",
        "else:\n",
        "    print(\"No notable upsets found in current filtered data.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Strategy Simulator Example Scenarios"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "teams = sorted(df[\"team1\"].dropna().unique())\n",
        "venues = sorted(df[\"venue\"].dropna().unique())\n",
        "\n",
        "team1 = teams[0]\n",
        "team2 = teams[1]\n",
        "venue = venues[0]\n",
        "\n",
        "scenario_a = ScenarioInput(\n",
        "    team1=team1,\n",
        "    team2=team2,\n",
        "    match_stage=\"Group Stage\",\n",
        "    venue=venue,\n",
        "    toss_winner=team1,\n",
        "    toss_decision=\"bat\",\n",
        "    elo_team1=1200,\n",
        "    elo_team2=1175,\n",
        "    team1_form_5=0.6,\n",
        "    team2_form_5=0.5,\n",
        "    team1_form_10=0.58,\n",
        "    team2_form_10=0.52,\n",
        "    h2h_win_pct=0.55,\n",
        ")\n",
        "scenario_b = ScenarioInput(**{**scenario_a.__dict__, \"toss_decision\": \"field\"})\n",
        "\n",
        "res_a = score_scenario(calibrated_model, build_scenario_features(scenario_a))\n",
        "res_b = score_scenario(calibrated_model, build_scenario_features(scenario_b))\n",
        "\n",
        "pd.DataFrame([\n",
        "    {\"scenario\": \"bat first\", **res_a},\n",
        "    {\"scenario\": \"field first\", **res_b},\n",
        "])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusions and MVP Status\n",
        "\n",
        "- The project now supports calibrated win probabilities, upset-risk labeling, and scenario-based strategy comparison.\n",
        "- Explainability has both global and local pathways for communication and trust.\n",
        "- Remaining MVP polish focuses on curated upset narratives and presentation assets (screenshots/GIF + results summary table).\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
