{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aeb91e07",
   "metadata": {},
   "source": [
    "# T20 Upset Radar and Strategy Simulator: Final Story\n",
    "\n",
    "This notebook is the portfolio-ready narrative for the MVP.\n",
    "\n",
    "It walks through:\n",
    "1. Data scope and quality snapshot\n",
    "2. Modeling and calibration baseline\n",
    "3. Upset-focused evaluation\n",
    "4. Explainability and local upset narratives\n",
    "5. Strategy simulator examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ac150f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
    "\n",
    "from src.config import PROCESSED_DIR\n",
    "from src.data_prep import (\n",
    "    assign_favorite_underdog_from_elo,\n",
    "    build_team1_win_target,\n",
    "    load_matches,\n",
    "    time_based_split,\n",
    ")\n",
    "from src.explain import build_counterfactual_explanation, rank_notable_upsets\n",
    "from src.features import build_pre_match_feature_frame\n",
    "from src.models import calibrate_classifier, evaluate_binary_model, train_logistic_baseline\n",
    "from src.simulation import ScenarioInput, build_scenario_features, score_scenario\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0b7931",
   "metadata": {},
   "source": [
    "## 1) Data Scope and Quality Snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa92032a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_matches()\n",
    "df = build_team1_win_target(df)\n",
    "df = assign_favorite_underdog_from_elo(df)\n",
    "\n",
    "quality_path = PROCESSED_DIR / \"data_quality_report.json\"\n",
    "quality = {}\n",
    "if quality_path.exists():\n",
    "    quality = json.loads(quality_path.read_text())\n",
    "\n",
    "print(\"Rows:\", len(df))\n",
    "print(\"Date range:\", df[\"date\"].min(), \"->\", df[\"date\"].max())\n",
    "print(\"Unique teams:\", df[\"team1\"].nunique())\n",
    "print(\"Overall upset rate:\", round(float(df[\"is_upset\"].mean()), 4))\n",
    "if quality:\n",
    "    print(\"Duplicate match_ids:\", quality.get(\"duplicate_match_id_count\", \"n/a\"))\n",
    "    print(\"Columns with missing values:\", quality.get(\"columns_with_missing_values\", \"n/a\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b364b1d0",
   "metadata": {},
   "source": [
    "## 2) Baseline Modeling and Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7a9d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, valid_df, test_df = time_based_split(df)\n",
    "\n",
    "X_train, y_train = build_pre_match_feature_frame(train_df)\n",
    "X_valid, y_valid = build_pre_match_feature_frame(valid_df)\n",
    "X_test, y_test = build_pre_match_feature_frame(test_df)\n",
    "\n",
    "base_model = train_logistic_baseline(X_train, y_train)\n",
    "calibrated_model = calibrate_classifier(base_model, X_valid, y_valid)\n",
    "test_metrics = evaluate_binary_model(calibrated_model, X_test, y_test)\n",
    "pd.Series(test_metrics).round(4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b16b3b5",
   "metadata": {},
   "source": [
    "## 3) Upset-Focused Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16232ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eval = test_df.copy()\n",
    "test_eval = assign_favorite_underdog_from_elo(test_eval)\n",
    "X_eval, _ = build_pre_match_feature_frame(test_eval)\n",
    "test_eval[\"team1_win_prob\"] = calibrated_model.predict_proba(X_eval)[:, 1]\n",
    "test_eval[\"pred_team1_win\"] = (test_eval[\"team1_win_prob\"] >= 0.5).astype(int)\n",
    "\n",
    "# Predict winner names in match orientation.\n",
    "test_eval[\"pred_winner\"] = test_eval.apply(\n",
    "    lambda r: r[\"team1\"] if r[\"pred_team1_win\"] == 1 else r[\"team2\"], axis=1\n",
    ")\n",
    "test_eval[\"pred_is_upset\"] = (test_eval[\"pred_winner\"] == test_eval[\"underdog_team\"]).astype(int)\n",
    "\n",
    "upset_cm = confusion_matrix(test_eval[\"is_upset\"], test_eval[\"pred_is_upset\"])\n",
    "upset_prf = precision_recall_fscore_support(\n",
    "    test_eval[\"is_upset\"], test_eval[\"pred_is_upset\"], average=\"binary\", zero_division=0\n",
    ")\n",
    "\n",
    "print(\"Upset confusion matrix (rows=true, cols=pred):\")\n",
    "print(upset_cm)\n",
    "print(\"Upset precision/recall/f1:\", tuple(round(float(x), 4) for x in upset_prf[:3]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f2048a",
   "metadata": {},
   "source": [
    "## 4) Explainability: Notable Upsets + Counterfactual View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c007340",
   "metadata": {},
   "outputs": [],
   "source": [
    "notable = rank_notable_upsets(df, top_n=8)\n",
    "notable[[\"date\", \"team1\", \"team2\", \"winner\", \"elo_diff\"]].head(8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419bbe98",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(notable) > 0:\n",
    "    top_case = notable.iloc[0]\n",
    "    case_df = df[(df[\"team1\"] == top_case[\"team1\"]) & (df[\"team2\"] == top_case[\"team2\"])].copy()\n",
    "    cols = [\n",
    "        \"team1\", \"team2\", \"match_stage\", \"venue\", \"toss_winner\", \"toss_decision\",\n",
    "        \"elo_team1\", \"elo_team2\", \"elo_diff\", \"team1_form_5\", \"team2_form_5\",\n",
    "        \"team1_form_10\", \"team2_form_10\", \"h2h_win_pct\"\n",
    "    ]\n",
    "    if not case_df.empty:\n",
    "        local_exp = build_counterfactual_explanation(calibrated_model, case_df[cols].head(1))\n",
    "        print(\"Base team1 win probability:\", round(local_exp[\"base_team1_win_prob\"], 4))\n",
    "        pd.DataFrame(local_exp[\"counterfactuals\"])\n",
    "    else:\n",
    "        print(\"No direct oriented row found for the top upset case.\")\n",
    "else:\n",
    "    print(\"No notable upsets found in current filtered data.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977f093b",
   "metadata": {},
   "source": [
    "## 5) Strategy Simulator Example Scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13b7e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "teams = sorted(df[\"team1\"].dropna().unique())\n",
    "venues = sorted(df[\"venue\"].dropna().unique())\n",
    "\n",
    "team1 = teams[0]\n",
    "team2 = teams[1]\n",
    "venue = venues[0]\n",
    "\n",
    "scenario_a = ScenarioInput(\n",
    "    team1=team1,\n",
    "    team2=team2,\n",
    "    match_stage=\"Group Stage\",\n",
    "    venue=venue,\n",
    "    toss_winner=team1,\n",
    "    toss_decision=\"bat\",\n",
    "    elo_team1=1200,\n",
    "    elo_team2=1175,\n",
    "    team1_form_5=0.6,\n",
    "    team2_form_5=0.5,\n",
    "    team1_form_10=0.58,\n",
    "    team2_form_10=0.52,\n",
    "    h2h_win_pct=0.55,\n",
    ")\n",
    "scenario_b = ScenarioInput(**{**scenario_a.__dict__, \"toss_decision\": \"field\"})\n",
    "\n",
    "res_a = score_scenario(calibrated_model, build_scenario_features(scenario_a))\n",
    "res_b = score_scenario(calibrated_model, build_scenario_features(scenario_b))\n",
    "\n",
    "pd.DataFrame([\n",
    "    {\"scenario\": \"bat first\", **res_a},\n",
    "    {\"scenario\": \"field first\", **res_b},\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc81c4fa",
   "metadata": {},
   "source": [
    "## Conclusions and MVP Status\n",
    "\n",
    "- The project now supports calibrated win probabilities, upset-risk labeling, and scenario-based strategy comparison.\n",
    "- Explainability has both global and local pathways for communication and trust.\n",
    "- Remaining MVP polish focuses on curated upset narratives and presentation assets (screenshots/GIF + results summary table).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44db4dc8",
   "metadata": {},
   "source": [
    "## 6) Missed Upsets Audit (High-Confidence Misses)\n",
    "\n",
    "This table highlights matches where the model gave the favorite a high win probability, but the underdog actually won.\n",
    "\n",
    "Interpretation focus:\n",
    "- larger `favorite_confidence` means the model was more surprised by the upset\n",
    "- these are useful cases for feature-gap analysis and future model improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7570f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "audit_df = test_eval.copy()\n",
    "audit_df[\"actual_winner\"] = audit_df[\"winner\"]\n",
    "audit_df[\"favorite_confidence\"] = audit_df.apply(\n",
    "    lambda r: r[\"team1_win_prob\"] if r[\"favorite_team\"] == r[\"team1\"] else (1 - r[\"team1_win_prob\"]),\n",
    "    axis=1,\n",
    ")\n",
    "audit_df[\"is_missed_upset\"] = (audit_df[\"is_upset\"] == 1) & (audit_df[\"pred_is_upset\"] == 0)\n",
    "\n",
    "missed_upsets = audit_df[audit_df[\"is_missed_upset\"]].copy()\n",
    "missed_upsets = missed_upsets.sort_values(\"favorite_confidence\", ascending=False)\n",
    "\n",
    "cols = [\n",
    "    \"date\",\n",
    "    \"team1\",\n",
    "    \"team2\",\n",
    "    \"actual_winner\",\n",
    "    \"favorite_team\",\n",
    "    \"underdog_team\",\n",
    "    \"favorite_confidence\",\n",
    "    \"team1_win_prob\",\n",
    "    \"is_upset\",\n",
    "    \"pred_is_upset\",\n",
    "    \"match_stage\",\n",
    "    \"venue\",\n",
    "]\n",
    "\n",
    "missed_upsets[cols].head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640a5569",
   "metadata": {},
   "source": [
    "## 7) Curated Upset Narratives (Top Cases)\n",
    "\n",
    "The table below converts top upset cases into short portfolio-style narratives.\n",
    "\n",
    "Selection logic:\n",
    "- rank by absolute ELO gap among upset matches\n",
    "- keep concise fields for storytelling (`stage`, `venue`, `favorite`, `winner`, `elo_gap`)\n",
    "- auto-generate a short explanation line for each case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3f2048",
   "metadata": {},
   "outputs": [],
   "source": [
    "narrative_cases = rank_notable_upsets(df, top_n=10).copy()\n",
    "\n",
    "if narrative_cases.empty:\n",
    "    print(\"No upset cases found for narrative generation.\")\n",
    "else:\n",
    "    narrative_cases = narrative_cases.copy()\n",
    "    narrative_cases[\"elo_gap\"] = narrative_cases[\"elo_diff\"].abs().round(1)\n",
    "\n",
    "    def build_case_narrative(row: pd.Series) -> str:\n",
    "        favorite = row.get(\"favorite_team\", \"favorite\")\n",
    "        winner = row.get(\"winner\", \"winner\")\n",
    "        stage = row.get(\"match_stage\", \"Unknown Stage\")\n",
    "        toss_decision = row.get(\"toss_decision\", \"unknown\")\n",
    "        gap = row.get(\"elo_gap\", \"n/a\")\n",
    "        return (\n",
    "            f\"At {stage}, {winner} beat favorite {favorite} despite an ELO gap of {gap}. \"\n",
    "            f\"Toss decision was '{toss_decision}', suggesting context pressure beyond baseline strength.\"\n",
    "        )\n",
    "\n",
    "    narrative_cases[\"narrative\"] = narrative_cases.apply(build_case_narrative, axis=1)\n",
    "\n",
    "    narrative_cols = [\n",
    "        \"date\",\n",
    "        \"team1\",\n",
    "        \"team2\",\n",
    "        \"winner\",\n",
    "        \"favorite_team\",\n",
    "        \"underdog_team\",\n",
    "        \"elo_gap\",\n",
    "        \"match_stage\",\n",
    "        \"venue\",\n",
    "        \"narrative\",\n",
    "    ]\n",
    "    narrative_cases[narrative_cols].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0c0e90",
   "metadata": {},
   "source": [
    "## 8) Cross-Case Patterns from Curated Upsets\n",
    "\n",
    "This quick summary aggregates the curated upset set to surface recurring contexts (stage, toss decision, venues)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f39379f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"narrative_cases\" in globals() and len(narrative_cases) > 0:\n",
    "    stage_summary = narrative_cases[\"match_stage\"].value_counts(dropna=False).rename(\"count\").to_frame()\n",
    "    toss_summary = narrative_cases[\"toss_decision\"].value_counts(dropna=False).rename(\"count\").to_frame()\n",
    "\n",
    "    print(\"Curated upset count:\", len(narrative_cases))\n",
    "    print(\"\\nTop stages in curated upsets:\")\n",
    "    display(stage_summary.head(10))\n",
    "    print(\"\\nToss decisions in curated upsets:\")\n",
    "    display(toss_summary)\n",
    "else:\n",
    "    print(\"Narrative cases not available.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
